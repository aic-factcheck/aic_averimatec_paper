%!TEX ROOT=../emnlp2023.tex

\section{System description}
\label{sec:system2026}

We adapted a system from~\citealt{ullrich-drchal-2025-aic} which extends on top of~\citealt{ullrich-etal-2024-aic}.
The cited paper describes the system in detail, with ablation studies and justifications of each step.
Our pipeline, depicted in Figure~\ref{fig:pipeline2026}, consists of precomputation, retrieval, and generation modules:

\begin{enumerate}[label=\roman*.]  % First-level: i., ii., iii.
\item Text-based retrieval module
\begin{enumerate}[label=\arabic*.]  % Second-level: 1., 2., 3.
    \item ~
\end{enumerate}
\item Image-based retrieval module
\begin{enumerate}[label=\arabic*.]  % Second-level: 1., 2., 3.
    \item ~
\end{enumerate}
\item Evidence \& label generation module
\begin{enumerate}[label=\arabic*.]  % Second-level: 1., 2., 3.
    \item ~
\end{enumerate}
\end{enumerate}

The main differences between this year's AIC \averitec{} system, opposed to last year's AIC AVeriTeC system, are the omission of knowledge store pruning in the precomputation step\footnote{The precomputed vector stores were required to be independent on claim text in \averitec{}.}, and, importantly, the choice of LLM.
\subsection{Model and parameter choices}
\label{sec:choices}
To produce our submission in the \averitec{} shared task, the following choices were made to deploy the pipeline from section~\ref{sec:system2026}:

\texttt{mxbai-embed-large-v1}~\cite{li-li-2024-aoe,emb2024mxbai} is used for the vector embeddings, and the maximum chunk size is set to 2048 characters, considering its input size of 512 tokens and a rule-of-thumb coefficient of 4 characters per token to exploit the full embedding input size and produce the smallest possible vector store size without neglecting a significant proportion of knowledge store text.

\texttt{FAISS}~\cite{douze2024faiss,johnson2019billion} index is used as the vector database engine, due to its simplicity of usage, exact search feature and quick retrieval times (sub-second for a single \averitec{} test claim).

$l=10, k=40, \lambda=0.75$ are the parameters we use for the MMR reranking, meaning that 40 chunks are retrieved, 10 sources are yielded after MMR-diversification, and the tradeoff between their similarity to the claim and their diversity is 3:1 in favour of the source similarity to the claim (explained in more detail in~\citealt{ullrich-etal-2024-aic}). 
