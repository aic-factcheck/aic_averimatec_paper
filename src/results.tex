%!TEX ROOT=../emnlp2023.tex

\section{Results and analysis}
\label{nothink}


\begin{table}[h]
\centering
\begin{tabular}{l
>{\centering\arraybackslash}p{.7cm} 
>{\centering\arraybackslash}p{.7cm} 
>{\centering\arraybackslash}p{.7cm} 
>{\centering\arraybackslash}p{.7cm}}
{\small{\textbf{System}}} &
\rotatebox{70}{\textbf{\footnotesize{Question Score}}} &
\rotatebox{70}{\textbf{\footnotesize{Evidence Score}}} &
\rotatebox{70}{\textbf{\footnotesize{Verdict Accuracy}}} &
\rotatebox{70}{\textbf{\footnotesize{Justification Score}}} \\
\hline
{\small{HUMANE}}        & {0.89} & {0.54} & {0.55} & {0.56} \\
{\small{ADA-AGGR}}      & 0.37 & 0.46 & 0.54 & 0.43 \\
{\textit{\small{AIC CTU (ours)} }}       & \textit{0.81} & \textit{0.33} & \textit{0.35} & \textit{0.30} \\
{\small{XxP}}           & 0.39 & 0.27 & 0.26 & 0.20 \\
{\small{teamName}}      & 0.66 & 0.23 & 0.26 & 0.22 \\
{\small{REVEAL}}        & 0.63 & 0.28 & 0.24 & 0.13 \\
{\small{fv}}            & 0.29 & 0.16 & 0.16 & 0.13 \\
\hline
{\small{Baseline}}      & 0.55 & 0.17 & 0.11 & 0.13 \\
\end{tabular}
\caption{System leaderboard showing performance metrics on \averitec{} test-split. Our system described in section~\ref{sec:system2026} is highlighted with \textit{italics}.}
\label{tab:leaderboard}
\end{table}

The final \averitec{} leaderboard is shown in table~\ref{tab:leaderboard}. Our system achieves a combined verdict score\footnote{Proportion of claims with a correct verdict \textit{and} an evidence score of at least 0.3 at the same time, see~\citealt{cao2025averimatecdatasetautomaticverification}.} of 0.35, with a near-SOTA question score of 0.81, mean evidence score of 0.35, and a justification score of 0.3.
Metrics are based on Ev2R~\cite{akhtar2024ev2r} recall scores with LLM as a judge.

While our system does not reach the very state of the art, it significantly outperforms the iterative agentic baseline~\cite{cao2025averimatecdatasetautomaticverification} and majority of other systems across the board, scoring a solid 3rd place.
To reveal directions for future improvements, we proceed to study what its main pitfalls are using the leaderboard metrics and our own reproductions of \averitec{} dev-split metrics.


\subsection{Bottlenecks}
\label{sec:bottlenecks}
\begin{table}[h]
\centering
\begin{tabular}{l
>{\centering\arraybackslash}p{.7cm} 
>{\centering\arraybackslash}p{.7cm} 
>{\centering\arraybackslash}p{.7cm} 
>{\centering\arraybackslash}p{.7cm}}
{\small{\textbf{Evidence format}}} &
\rotatebox{70}{\textbf{\footnotesize{Question Score}}} &
\rotatebox{70}{\textbf{\footnotesize{Evidence Score}}} &
\rotatebox{70}{\textbf{\footnotesize{Verdict Accuracy}}} &
\rotatebox{70}{\textbf{\footnotesize{Justification Score}}} \\
\hline
{\small{Answer only}}   & \textbf{0.86} & 0.27 & 0.31 & 0.28 \\
{\textit{\small{Question + Answer}}}   & \textit{0.84} & \textit{0.33} & \textit{\textbf{0.39}} & \textit{0.31} \\
{\small{Declarative evidence}}   & 0.82 & \textbf{0.35} & 0.38 & \textbf{0.32} \\
\end{tabular}
\caption{Ablation study tweaking the evidence generation format from section~\ref{sec:system2026}, iii. Scheme used in final submission is in italics.}
\label{tab:ablation}
\end{table}

Looking at our standing in the leaderboard from table~\ref{tab:leaderboard}, the main bottleneck appears to be our system's \textit{evidence score}, computed using Ev2R recall. Despite the question score shows promising 81\% our lack in evidence score then propagates further to the verdict and justification scores as well.
Part of this problem could be attributed to our system's legacy evidence format geared more towards AVeriTeC 1 and 2 shared tasks -- an \textit{evidence} is generated as a QA pair, of a question fact-checker would ask themselves during the task, and an answer they would arrive to, grounded in an URL-referred source, whereas in \averitec{} evaluation scheme, the evidence is a self-contained declarative sentence with pointers to relevant images.

Table~\ref{tab:ablation} lists three approaches we took to address this discrepancy -- in our first approach, we disregarded this entirely and only listed the generated answers as the \averitec{} evidence, in our second approach -- which is also the one we have submitted to the final leaderboard (table~\ref{tab:leaderboard}) -- we have concatenated the question and answer together to obtain each evidence string, appending a \texttt{[IMG\_1]} tag and a base64-encoded image in metadata where an image-related source was used. 

To see whether this can be improved upon, we have also experimentally implemented a 3rd approach, referred to as \say{declarative evidence} in table~\ref{tab:ablation}, in which we have directly prompted the LLM to generate a self-contained declarative evidence text with pointers to used images.
Although this approach was experimental and not free of its own glitches (resulting in a malformed image pointers and \texttt{[IMG\_1]} tag being erroneously used in other generic fields, such as justification and questions), it shows promissing results, surpassing our \textit{Question+Answer} approach by encouraging 2\% in the evidence score, even before adjusting its prompt to iron out the glitches.

Another bottleneck could be possible discrepancies in our image-evidence usage -- looking closer at the ablation study in table~\ref{tab:ablation}, the \say{Answer only} approach stays too close behind its more advanced alternatives.
This finding raises concerns, since the answer-only approach does not use \textit{any} \texttt{[IMG\_1]} tags, yet per~\citealt{cao2025averimatecdatasetautomaticverification}, 53.9\% of the \averitec{} evidence should be annotated using reverse image search, with 1.6\% using the image itself as the answer. 
This is to be investigated in future works, as even a small discrepancy in the way our system presents its image sources and how the \averitec{} evaluator assumes to receive them may have a tremendous impact on the final score.


\subsection{Cost analysis}
The scheme from section~\ref{sec:system2026} uses a single RIS request per claim image (one claim may feature multiple images, but the vast majority features exactly 1 image in \averitec{}) -- using Serper, this search comes at a cost of 3 credits, totalling at \$0.003 using the least-discounted bulk pricing (\$50 for 50K Serper credits).

The markdown scraping was performed using the Firecrawl API, which at its hobby tier charges \$0.006 per scraped page, with 20,000 free scraping tasks for education emails -- in the worst case scenario of multiple claim images in a single claim, each with full 9 RIS results older than the claim date that can be scraped\footnote{Which is not usually the case, as at least some proportion of results typically come from Meta's scraping-protected social media} and no discount, this amounts to \$0.05 per image.
To avoid this cost, however,  we suggest using a free scraper instead, such as the Trafilatura library which was used to produce the \averitec{} offline knowledge stores and our system does not show any noticeable problems ingesting its outputs.

The Generation module LLM results were computed using the OpenAI Batch API, with GPT-5.1 as the backbone model.
On average, 11K completion input tokens were given to the model and 1150 tokens of output were generated per \averitec{} claim using our system from section~\ref{sec:system2026}, at an average cost of \$0.013 per claim.